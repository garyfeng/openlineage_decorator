{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be143d26",
   "metadata": {},
   "source": [
    "# OpenLineage Python Integration\n",
    "\n",
    "This notebook demonstrates the use of python decorator to emit OpenLineage events for python and pySpark functions. \n",
    "\n",
    "## Problem and Solutions\n",
    "\n",
    "[OpenLineage](https://github.com/OpenLineage/OpenLineage) is an open standard and an open-source implementation supported by the Linux Foundation for data observability and data lineage tracking. It defines a standard for data lineage events (see schema at https://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineage.json) that data processing pipelines send to a centralized RESTful service, backed by a Postgres database. This data can be queried and visualized, e.g., using the [open-source Marquez UI](https://github.com/MarquezProject/marquez). The goal is to log every data transformation and transaction, with versioning history. \n",
    "\n",
    "The challenge is to make this process unintrusive and ubiquitous. In our use case (Spark/pySpark, Delta Lake on Databricks), there are two approaches to integrate OpenLineage:\n",
    "\n",
    "1. Using SparkListener, see https://openlineage.io/blog/openlineage-spark/. This involves adding a custom SparkListner to the Spark/Databricks environment, which automatically reports low-level Spark operations and file I/O. This is demonstrated in a separate notebook [OpenLineage-Spark Demo.ipynb](/notebooks/notebooks/OpenLineage-Spark%20Demo.ipynb).\n",
    "    - The advantage is that once set up, it tracks everything it can track without any additional work.\n",
    "    - The disadvantage is also the lack of control -- you can't refine what you want to track; e.g., instead of your pySpark code, it only has access to the low-level Spark execution plans. \n",
    "2. Using Python and the RESTful API. \n",
    "    - Advantage is that you can log anything you want.\n",
    "    - The drawback is that you have to specify what you want to log.\n",
    "    \n",
    "The goal of this demo is to show how we can simplify the Python/API approach, by using python decorators. The idea is to pack all OpenLineage functions in a decorator function, so that the user only need to do the following to transformations that requires loggin:\n",
    "\n",
    "```python\n",
    "from openlineage-decor import OpenLineageDecor\n",
    "\n",
    "# set up OL config\n",
    "ol_config = {\n",
    "    # set up your OL URL, namespace, password, etc.\n",
    "}\n",
    "\n",
    "# defining the transformation using the decorator\n",
    "@OpenLineageDecor(ol_config)\n",
    "def my_transformation(df1, df2):\n",
    "    # ...\n",
    "    return output_df\n",
    "\n",
    "# calling this function will generate OL events\n",
    "df = my_transformation(df1, df2)\n",
    "```\n",
    "\n",
    "And the decorator `@openlineage` will take care all the logging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0191005",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This demo uses `docker-compose` to run a set of connected services:\n",
    "- the Marquez OpenLineage API\n",
    "- the Marquez web server/viz UI\n",
    "- the Postgres database as the backend\n",
    "- a Jupyter notebook service with Spark 3.1+\n",
    "\n",
    "To start, clone my branch of `openlineage` where I fixed https://github.com/OpenLineage/OpenLineage/issues/633. It's a very easy fix you can just patch the official repo if you want to. See the issue link.\n",
    "\n",
    "```sh\n",
    "git clone https://github.com/garyfeng/OpenLineage.git\n",
    "cd OpenLineage/integration/spark\n",
    "mkdir -p docker/notebooks\n",
    "# copy these notebooks to the above folder if they do not exist there\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "Start Jupyter at http://127.0.0.1:8888. You may need to look into the server logs to find the passcode to access the notebooks.\n",
    "\n",
    "To view the web UI, in a different terminal\n",
    "\n",
    "```sh\n",
    "docker run --network spark_default -p 3000:3000 -e MARQUEZ_HOST=marquez-api -e MARQUEZ_PORT=5000 --link marquez-api:marquez-api marquezproject/marquez-web\n",
    "```\n",
    "\n",
    "Then open Marquez at http://127.0.0.1:3000/. Look for the `namespace` \n",
    "\n",
    "# Best Practice for Development\n",
    "\n",
    "Here are the best practice for using this pattern with OpenLineage:\n",
    "\n",
    "OpenLineage code/config management:\n",
    "\n",
    "- The OpenLineage decorator function should be part of a library that is imported for each notebooks or work\n",
    "- The OpenLineage code should be versioned\n",
    "- The OpenLineage configurations (server URL, schemas, etc.) should be managed, with cutomization in each notebook/work\n",
    "\n",
    "In each Notebook:\n",
    "- Import OpenLineage decor lib \n",
    "- Initialize the instance of the OpenLineage client\n",
    "  - with the Notebook Name or Work Name as the `namespace`\n",
    "  - confirm the configuration is working\n",
    "- Register global variables (typically limited to Spark/pandas Dataframes). This will emit an initial list of `dataset` records to the OL backend\n",
    "- Use a function for all data manipulations, especially with dataframes\n",
    "  - Use the `@openlineage` decorator for any function you want to track\n",
    "  - Name your function descriptively; the function name will be the name of the `job`/`run` in OL\n",
    "  - Use the `docstring` to briefly describe the purpose of the function; this will be tracked as well.\n",
    "  - The source of the function will also be tracked, including all comments.\n",
    "  - Keep your input and output simple, e.g., avoid returning a tuple or something too complex. Typically a function will take one or more dataframes as input (maybe with a few other parameters as options) and return a dataframe, series, or scalar. \n",
    "- I/O sideeffects (such as saving data to file) is NOT tracked with this method (YET; TO-DO)\n",
    "  - As an interim solution, you can use `____` to send custom meessage in the code \n",
    "  \n",
    "```python\n",
    "from openlineage_decor import openlineage, OpenLineage, OPENLINAGE_CONFIG\n",
    "\n",
    "# change default config\n",
    "OPENLINAGE_CONFIG[\"url\"] = \"http://host:port/api/v1/openlineage\"\n",
    "OPENLINAGE_CONFIG[\"namespace\"] = \"Notebook Name\"\n",
    "# set up the ol client\n",
    "ol_client = OpenLineage(OPENLINAGE_CONFIG)\n",
    "# test ol_client\n",
    "pass\n",
    "# pass the ol_client to the openlineage decor function\n",
    "OPENLINAGE_CONFIG[\"client\"] = ol_client\n",
    "# register global dataframes; \n",
    "df_list = get_global_dfs()\n",
    "ol_client.register_dfs(df_list)\n",
    "\n",
    "```\n",
    "\n",
    "# TO-DOs\n",
    "\n",
    "- add decorators for register_data_sources: to register data sources and data frames with the metadata store. Not sure what we will use as the unique identifier -- for python/pyspark/pandas it seems `id()` is just fine, because it is unique within the session. Maybe `session+id()`?\n",
    "  - so long as we use a unique ID to track data the DAG should connect all `datasets`/`jobs` within the `Namespace`\n",
    "  - but the downside is that we will have too many data objects in the OL registery (duplicated for every run). The DAG is based on all runs of all objects. \n",
    "  - We need to distinguish between dataset `name` vs `version`, where the `id(df)` is like a version number. \n",
    "- add code version tracking\n",
    "- add support for RDD, Pandas dataframe, series, etc. \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce11c33",
   "metadata": {},
   "source": [
    "# Start Spark Session\n",
    "\n",
    "We will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbea3d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/02 01:59:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/02 01:59:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import urllib.request\n",
    "\n",
    "# Set these to your own project and bucket\n",
    "spark = (SparkSession.builder.master('local').appName('openlineage_python_API')\n",
    "             .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a352e",
   "metadata": {},
   "source": [
    "# OpenLineage Python: Manual Testing\n",
    "\n",
    "In this demo we actually do not use the python client. We use python `request` library and the OpenLineage API directly for clarity. We can consider using the official python client in the future. \n",
    "\n",
    "We will do some manual testing. Run the following a few cells, and check the Marquez UI at http://127.0.0.1:3000/. Look for `namespace` \"gary-namespace\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c084af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openlineage-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c7e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://marquez_api:5000/api/v1/lineage'\n",
    "headers = {\"charset\": \"utf-8\", \"Content-Type\": \"application/json\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5027a056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\"{\n",
    "        \"eventType\": \"START\",\n",
    "        \"eventTime\": \"2020-12-28T19:52:00.001+10:00\",\n",
    "        \"run\": {\n",
    "          \"runId\": \"d46e465b-d358-4d32-83d4-df660ff614dd\"\n",
    "        },\n",
    "        \"job\": {\n",
    "          \"namespace\": \"gary-namespace\",\n",
    "          \"name\": \"my-job\"\n",
    "        },\n",
    "        \"inputs\": [{\n",
    "          \"namespace\": \"gary-namespace\",\n",
    "          \"name\": \"my-input\"\n",
    "        }],  \n",
    "        \"producer\": \"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client\"\n",
    "      }\"\"\"\n",
    "r = requests.post(url, data=data, headers=headers)\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17cc68f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = \"\"\"{\n",
    "        \"eventType\": \"COMPLETE\",\n",
    "        \"eventTime\": \"2020-12-28T20:52:00.001+10:00\",\n",
    "        \"run\": {\n",
    "          \"runId\": \"d46e465b-d358-4d32-83d4-df660ff614dd\"\n",
    "        },\n",
    "        \"job\": {\n",
    "          \"namespace\": \"gary-namespace\",\n",
    "          \"name\": \"my-job\"\n",
    "        },\n",
    "        \"outputs\": [{\n",
    "          \"namespace\": \"gary-namespace\",\n",
    "          \"name\": \"my-output\",\n",
    "          \"facets\": {\n",
    "            \"schema\": {\n",
    "              \"_producer\": \"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client\",\n",
    "              \"_schemaURL\": \"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/spec/OpenLineage.json#/definitions/SchemaDatasetFacet\",\n",
    "              \"fields\": [\n",
    "                { \"name\": \"a\", \"type\": \"VARCHAR\"},\n",
    "                { \"name\": \"b\", \"type\": \"VARCHAR\"}\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        }],     \n",
    "        \"producer\": \"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client\"\n",
    "      }\"\"\"\n",
    "\n",
    "r = requests.post(url, data=data2, headers=headers)\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7648f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808dc000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ef1b84c",
   "metadata": {},
   "source": [
    "# Wrappers for OpenLineage Trackers\n",
    "\n",
    "The idea is to use python decorators to log OpenLineage events when running pySpark transformations. \n",
    "\n",
    "Assuming major transformations are coded as pySpark functions (as part of a library), and further assuming that they follow the pattern of having an input df and an output df, we can use an decorator to \n",
    "- extract `run` info, with python source code of the function and all parameters/configurations, timing and start/completion\n",
    "- extract `data` info, with input and output data frames\n",
    "- post to OpenLineage server, with error logging if needed \n",
    "\n",
    "Need to plan the granularity of the logs -- not individual operations but a block of code or a notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69737898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time, uuid, json\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import inspect\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# making this a decorator factor to take an input of the namespace\n",
    "def openlineage(namespace):\n",
    "\n",
    "    def openlineage_decor(func):\n",
    "        \"\"\"Report OpenLineage events\"\"\"\n",
    "        ol_event_producer = \"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client\"\n",
    "        ol_producer = \"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client\"\n",
    "        ol_schemaURL = \"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/spec/OpenLineage.json#/definitions/SchemaDatasetFacet\"\n",
    "        ol_url = 'http://marquez_api:5000/api/v1/lineage'\n",
    "        ol_headers = {\"charset\": \"utf-8\", \"Content-Type\": \"application/json\"}\n",
    "        ol_runId = str(uuid.uuid4())\n",
    "        ol_namespace = namespace # from the decorator factory\n",
    "        ol_job_name = func.__name__\n",
    "        \n",
    "        def ol_send_event(ol_data):\n",
    "            \"\"\"Posting the OpenLineage event\"\"\"\n",
    "            # print(ol_data)\n",
    "            r = requests.post(ol_url, data=ol_data, headers=ol_headers)\n",
    "            if not r.ok:\n",
    "                print(\"Failed to post endEvent: {ol_data}\")\n",
    "            \n",
    "        def ol_data_description(var):\n",
    "            \"\"\"Given a {var_name: var}, return an object containing data description for the object\n",
    "            param var: a dict in the form of {v_name: v_obj}, where v_name is a string, and v_object is \n",
    "                        the actual reference to the object.\n",
    "            returns: a dict in the form of the OpenLineage DataSchema \n",
    "            \"\"\"\n",
    "            assert isinstance(var, dict)\n",
    "            assert len(var.keys())==1\n",
    "            v_name = list(var.keys())[0]\n",
    "            v_obj = list(var.values())[0]\n",
    "            output = None\n",
    "\n",
    "            # Detect if it's a DF, report the DF schema\n",
    "            try:\n",
    "                if isinstance(v_obj, DataFrame):\n",
    "                    # Spark Dataframe\n",
    "                    # name = \"df_\"+id(df), where id(df) is a reference to the memory allocation;\n",
    "                    # the resulting name is the same for all references to the same df: df1 = df\n",
    "                    # this helps to link the DAG in marquez to track data across jobs\n",
    "                    data_fields = [ {\"name\":v, \"type\":t, \n",
    "                                     \"description\": \"\"} for v,t in v_obj.dtypes]\n",
    "                    output = {\n",
    "                      \"namespace\": ol_namespace,\n",
    "                      \"name\": \"df_{}\".format(id(v_obj)), #switching to global id() of the input\n",
    "                      \"facets\": {\n",
    "                          \"schema\": {\n",
    "                            \"fields\": data_fields,\n",
    "                            \"_producer\": ol_producer,\n",
    "                            \"_schemaURL\": ol_schemaURL,\n",
    "                          }\n",
    "                      }\n",
    "                    }                    \n",
    "                else:\n",
    "                    # all other data types, assuming scalar\n",
    "                    # name = v_name, field is for this var\n",
    "                    output = {\n",
    "                      \"namespace\": ol_namespace,\n",
    "                      \"name\": v_name,\n",
    "                      \"facets\": {\n",
    "                          \"schema\": {\n",
    "                            \"fields\": [{\"name\": v_name, \"type\": type(v_obj).__name__, \n",
    "                                        \"description\": \"\"}],\n",
    "                            \"_producer\": ol_producer,\n",
    "                            \"_schemaURL\": ol_schemaURL,\n",
    "                          }\n",
    "                      }\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(e) # in case of empty parameters, etc.\n",
    "\n",
    "            return output\n",
    "\n",
    "        def ol_run_description(func):\n",
    "            \"\"\"Given a funcion, generate an OpenLineage Run facet description.\n",
    "            We use the name of the function as the run/job name; we include\n",
    "            the input parameters, source code, docstring, and path of the python \n",
    "            file in the run record.\n",
    "            \"\"\"\n",
    "            # Job/run info\n",
    "            source_code=source_file=\"\"\n",
    "            try:\n",
    "                source_code = inspect.getsource(func)\n",
    "                source_file = inspect.getsourcefile(func)\n",
    "            except:\n",
    "                pass\n",
    "            run_info = {\n",
    "              \"runId\": ol_runId,\n",
    "              \"description\":inspect.getdoc(func),\n",
    "              \"facets\": {# TO-DO: add git URL and version info\n",
    "                  \"function\": {\n",
    "                    \"name\": func.__name__,\n",
    "                    \"docstring\": inspect.getdoc(func),\n",
    "                    \"source\": source_code, \n",
    "                    \"sourceFile\": source_file,\n",
    "                    \"signature\": list(inspect.signature(func).parameters),\n",
    "                    #\"params\": param,\n",
    "                    \"_producer\": ol_producer,\n",
    "                    \"_schemaURL\": \"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/spec/OpenLineage.json#/definitions/JobFacet\",\n",
    "                  }\n",
    "              }\n",
    "            }         \n",
    "            return run_info\n",
    "\n",
    "        def ol_beforerun(*args, **kwargs):\n",
    "            \"\"\"Emitting the OpenLineage START event with info on the job/run and input\"\"\"\n",
    "            # get parameter list to func\n",
    "            param1 = [{k:v} for k,v in zip(inspect.signature(func).parameters, args)]\n",
    "            param2 = [{key: value} for key, value in kwargs.items()]\n",
    "            param = param1 + param2\n",
    "            # make each para an input facet; this may not make sense when params are scalar\n",
    "            # but when the inputs are dataframes we can better track the data\n",
    "            input_list = [ol_data_description(p) for p in param]\n",
    "\n",
    "            # get run info\n",
    "            run_facet = ol_run_description(func)\n",
    "\n",
    "            startEvent = json.dumps({\n",
    "                \"eventType\": \"START\",\n",
    "                \"eventTime\": datetime.now().isoformat(sep='T', timespec='milliseconds'),\n",
    "                \"run\": run_facet,\n",
    "                \"job\": {# TO-DO: what more do we need?\n",
    "                  \"namespace\": ol_namespace,\n",
    "                  \"name\": ol_job_name\n",
    "                },\n",
    "                \"inputs\": input_list,  \n",
    "                \"producer\": ol_event_producer\n",
    "            })\n",
    "            ol_send_event(startEvent)\n",
    "            return\n",
    "\n",
    "        def ol_afterrun(value):\n",
    "            \"\"\"Emitting the OpenLineage COMPLETE event with info on the job/run and output\"\"\"\n",
    "            endEvent = json.dumps({\n",
    "                \"eventType\": \"COMPLETE\",\n",
    "                \"eventTime\": datetime.now().isoformat(sep='T', timespec='milliseconds'),\n",
    "                \"run\": {\n",
    "                  \"runId\": ol_runId\n",
    "                },\n",
    "                \"job\": {\n",
    "                  \"namespace\": ol_namespace,\n",
    "                  \"name\": ol_job_name\n",
    "                },\n",
    "                \"outputs\": [ol_data_description({\"{}_output\".format(ol_job_name): value})],     \n",
    "                \"producer\": ol_event_producer\n",
    "            })\n",
    "            ol_send_event(endEvent)\n",
    "            return\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        def emit_ol_event(*args, **kwargs):\n",
    "            # before run\n",
    "            ol_beforerun(*args, **kwargs)\n",
    "\n",
    "            # now actually run the function and time it\n",
    "            start_time = time.perf_counter()    # 1\n",
    "            value = func(*args, **kwargs)\n",
    "            end_time = time.perf_counter()      # 2\n",
    "\n",
    "            # after run\n",
    "            ol_afterrun(value)\n",
    "\n",
    "            run_time = end_time - start_time    # 3\n",
    "            # print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n",
    "            return value\n",
    "\n",
    "        # return the decorator function\n",
    "        return emit_ol_event\n",
    "    \n",
    "    return openlineage_decor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2453439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with a dummy function\n",
    "@openlineage(\"test2_namespace\")\n",
    "def waste_some_time(num_times, dummy, **kwargs):\n",
    "    \"\"\"Just a dummy function\n",
    "    \n",
    "    param num_times: input for # of iterations\n",
    "    param dummy: useless parameter\n",
    "    returns: a dummy Spark DataFrame\n",
    "    \"\"\"\n",
    "    for _ in range(num_times):\n",
    "        sum([i**2 for i in range(10000)])\n",
    "    return spark.sparkContext.parallelize([(\"foo\", 1)]).toDF()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@openlineage(\"test2_namespace\")\n",
    "def waste2(df:DataFrame, **kwargs):\n",
    "    \"\"\"Convert a Spark DF to Pandas and then back\"\"\"\n",
    "    pdf = df.select(\"*\").toPandas()\n",
    "    return spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "537c81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will trigger the first part of the job\n",
    "df = waste_some_time(10, 0, junk=2, other=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df2d44f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will trigger the second part of the job, which will be connected with the firt part via the shared df\n",
    "waste2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3911e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can chain them together. Still chained by the temp df that is the output of one and input for the other. \n",
    "# in OL, the named df and temp df are treated the same, all identified by their id()\n",
    "waste2(waste_some_time(10, 0, anther=\"useless\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e00c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12f5d8e3",
   "metadata": {},
   "source": [
    "# Class-based approach\n",
    "\n",
    "We create a class for OpenLinearDecorator, and initiate one for each.\n",
    "\n",
    "Typical approach uses a python `class` as a decorator uses the `.__call__()` function in the class to do the decorating function. This allows one to first define a decorator class\n",
    "\n",
    "```python\n",
    "class OpenLineage(object):\n",
    "    def __init__(self, param):\n",
    "        ...\n",
    "    def __call__(self, func):\n",
    "        ...\n",
    "        def emit_event(*args, **kwargs):\n",
    "            # do before\n",
    "            val= func(*args, **kwargs)\n",
    "            # do after\n",
    "            return val\n",
    "        return emit_event\n",
    "# define a fun\n",
    "@OpenLineage\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "```\n",
    "\n",
    "But keep in mind that every time `@OpenLineage` is called, it creates an instance that will **not** be reused. So if you need to initiate the decorator class with parameters/configurations, you need to do that every time (`@OpenLineage(config)`).\n",
    "\n",
    "The following is one approach, where we define a generic class, instantiate an instance with configurations, and use the instance as decorator for subsequent func definitions.\n",
    "- pros:\n",
    "  - us class and instances; allow multiple decorators to be initiated with different parameters, such as URLs and Namespaces.\n",
    "  - don't need to include config for every decorators (if we had a `class OpenLineage` as the decorator class that takes parameters, we'd need to do `@OpenLineage(config)` every time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe380f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiated with url=http://localhost:500, namespace=test_namespace\n"
     ]
    }
   ],
   "source": [
    "class OpenLineage(object):\n",
    "    def __init__(self, url, namespace):\n",
    "        self.url = url\n",
    "        self.namespace = namespace\n",
    "        # print at instance initiation\n",
    "        print(\"Initiated with url={}, namespace={}\".format(self.url, self.namespace))\n",
    "        \n",
    "    def ol_send_event(self, event):\n",
    "        \"\"\"Mock function for sending OpenLineage event data\"\"\"\n",
    "        print(\"OL: {}\".format(event))\n",
    "        \n",
    "    def track(self, func):\n",
    "        \"\"\"This is a decorator function. Given a function that does data transformation'\n",
    "        , this adds OpenLineage even tracking before and after the run.\"\"\"\n",
    "        # print message in decorator stage\n",
    "        self.ol_send_event(\"JOB: job={}, url={}, namespace={}\".format(func.__name__, self.url, self.namespace))\n",
    "        \n",
    "        @functools.wraps(func)\n",
    "        def emit_ol_event(*args, **kwargs):\n",
    "            # before run\n",
    "            # print at runtime\n",
    "            self.ol_send_event(\"RUN INIT: with url={}, namespace={}\".format(self.url, self.namespace))\n",
    "            self.ol_send_event(\"RUN START: calling {}{}\".format(func.__name__, args))\n",
    "            # now actually run the function and time it\n",
    "            value = func(*args, **kwargs)\n",
    "            # after run\n",
    "            self.ol_send_event(\"RUN COMPLETE: returning data type {}\".format(type(value).__name__))\n",
    "            return value\n",
    "        # return the decorator function\n",
    "        return emit_ol_event\n",
    "    \n",
    "    def register(self, *args):\n",
    "        \"\"\"Mock to register a dataset. Notes:\n",
    "        1). This only registers data in the globals() scope.\n",
    "        2). The registration is done by id(var), and will register all variables referring to this object.\n",
    "        TO-DO: to take variable parameters, *arg \"\"\"\n",
    "        arg_ids = [id(x) for x in args]\n",
    "        l = globals()\n",
    "        dataset = [(x, type(l[x]).__name__, id(l[x])) for x in l.keys() \n",
    "                   if id(l[x]) in arg_ids and not x.startswith(\"_\")]\n",
    "        self.ol_send_event(\"DATASET: {}\".format(dataset))\n",
    "\n",
    "# instantiate with parameters\n",
    "ol = OpenLineage(url=\"http://localhost:500\", namespace=\"test_namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c7f3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: JOB: job=add, url=http://localhost:500, namespace=test_namespace\n"
     ]
    }
   ],
   "source": [
    "@ol.track\n",
    "def add(a,b):\n",
    "    print(\"running add({}, {})\".format(a,b))\n",
    "    return a+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebd6a8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: RUN INIT: with url=http://localhost:500, namespace=test_namespace\n",
      "OL: RUN START: calling add(1, 2)\n",
      "running add(1, 2)\n",
      "OL: RUN COMPLETE: returning data type int\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ba36f",
   "metadata": {},
   "source": [
    "## Register Global Data Objects\n",
    "\n",
    "The Python decorator approach tracks data transformation functions, which corresponds to `jobs` or `runs` in OpenLineage. How do we track data objects such as data frames?\n",
    "\n",
    "This is implicitly done in the decorator code when we assigned dataset schema for input and output of a job/run. Basically,\n",
    "- for Spark/Pandas dataframes, we track the identity of the df (in this run) with a temporary name `\"df_\"+str(id(df))`. The reason for this, as opposed to using the \"name\" of the df is that all dfs are by reference. You can have multiple dfs pointing to the same data. You can also reassign a df to point to a different data object. The DF name is actually not a unique identifier. We want to track the actual data object. \n",
    "  - we have a field schema based on the column names and data types.\n",
    "- for other data types, particular built-in types, we use the variable name as data name, and use the var name and type as the field schema.\n",
    "\n",
    "This does not necessarily cover all other data objects we may want to track -- e.g., ones that are not manipulated within a function that is tracked by the decorator, or in an interaction session. \n",
    "\n",
    "We can't use a decorator because variables are not callable. A complementary way is to declare the intention to track a data object by `.register(data)`. We use the same OpenLineage instance but use a method call `.register()` for each variable we want to register. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b116e3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: DATASET: [('a', 'int', 140142062703056)]\n"
     ]
    }
   ],
   "source": [
    "# interesting to see how even built-in vars are by reference.\n",
    "a=12352523\n",
    "ol.register(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dd53dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: DATASET: [('a', 'int', 140142062703056), ('b', 'int', 140142062703056), ('c', 'int', 140142062703056)]\n"
     ]
    }
   ],
   "source": [
    "# in this case b and c points to a\n",
    "b=a; c=b\n",
    "ol.register(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d018095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: DATASET: [('a', 'int', 140143782160720)]\n",
      "OL: DATASET: [('a', 'int', 140143782160720), ('b', 'int', 140142062703056), ('c', 'int', 140142062703056)]\n"
     ]
    }
   ],
   "source": [
    "a=2\n",
    "ol.register(a)\n",
    "ol.register(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56af35b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: DATASET: [('a', 'int', 140143782160720), ('b', 'int', 140143782160720), ('RandomVar', 'int', 140143782160720)]\n"
     ]
    }
   ],
   "source": [
    "# does everything that points to int2 have the same id now?\n",
    "b=2\n",
    "RandomVar=int(\"2\")\n",
    "ol.register(b, RandomVar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78fe2808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: DATASET: [('RandomVar', 'int', 140142062703472), ('z', 'int', 140142062703536)]\n"
     ]
    }
   ],
   "source": [
    "# this fails to work when you have large numbers.\n",
    "z=2122344\n",
    "RandomVar=int(\"2122344\")\n",
    "ol.register(z, RandomVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241029f3",
   "metadata": {},
   "source": [
    "Now data frames are always by reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8068afa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: DATASET: [('df', 'DataFrame', 140142062575328)]\n"
     ]
    }
   ],
   "source": [
    "ol.register(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11485f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: DATASET: [('df', 'DataFrame', 140142062575328), ('df1', 'DataFrame', 140142062575328)]\n"
     ]
    }
   ],
   "source": [
    "df1 = df\n",
    "ol.register(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5501253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: DATASET: []\n"
     ]
    }
   ],
   "source": [
    "# nothing is registered since the output of waste2(df) is a temp var that starts with _ and gets filtered out\n",
    "ol.register(waste2(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d01d8442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL: DATASET: [('df2', 'DataFrame', 140143128052352)]\n"
     ]
    }
   ],
   "source": [
    "# this named df works. \n",
    "df2 = waste2(df)\n",
    "ol.register(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf02ff31",
   "metadata": {},
   "source": [
    "## Monitoring global variables\n",
    "\n",
    "The idea here is to automatically monitor variables/objects of interest in the relevant scope, and register. If this is done at all, it should be done only for specific data types such as Spark/Pandas data frames and file handlers, non-temp variables only. In addition, we should not rely on scanning the `globals()` frequently. This should be triggered only after some relevant actions. \n",
    "\n",
    "In general, use `global()`. Avoid `gc.get_objects()`, which runs the risk of interfering with the gc process and memory leak. If you really want to get into gc, consider using `weakref` in reporting, but I haven't found a reason to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples to get built-in types, to identify non-built-in types such as data frames, etc.\n",
    "import builtins\n",
    "builtin_types = [getattr(builtins, d) for d in dir(builtins) if isinstance(getattr(builtins, d), type)]\n",
    "[t for t in builtin_types if not t.__name__[0].isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf42571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate temporary vars that will be gc-ed. \n",
    "set([x for x in locals().keys() if not x.startswith(\"_\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52471b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depending on the gc process, this may return temporary dataframes that starts with '_'\n",
    "[(x, type(l[x])) for x in l.keys() if id(l[x])==id(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46fb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generally not advised. But you can use gc to find an object by its id(), \n",
    "# which is related to the memory allocation. Know what you doing before you go there. \n",
    "\n",
    "import gc\n",
    "\n",
    "def deref(id_):\n",
    "    f = {id(x):x for x in gc.get_objects()}\n",
    "    return f[id_]\n",
    "\n",
    "df1=df\n",
    "\n",
    "# this gets you the object, but not the name reference, which may not be unique. \n",
    "# you can search through globals() for objects pointing to this id() to find all co-references\n",
    "# but be careful that stuff in the gc may get erased any time; the same id() may not give you\n",
    "# the same object. \n",
    "deref(id(df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14782672",
   "metadata": {},
   "source": [
    "# Using Notebook Magic to log OpenLineage cells\n",
    "\n",
    "See custom cell magic for Jupyter notebooks https://ipython.readthedocs.io/en/stable/config/custommagics.html\n",
    "\n",
    "The idea is to define `cell magic` commands to generate `OpenLineage` events before and after running the cell. \n",
    "- The `cell magic` returns the content of the cell as a string. We can run the python script with a separate python process and capture the results to pass back. Not sure about security and access to Spark session, etc. \n",
    "\n",
    "\n",
    "This only works when the user runs the notebook via the UI. Not sure it works when Databricks \"runs\" the notebook automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed090e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "\n",
    "@register_line_magic\n",
    "def lmagic(line):\n",
    "    print(\"my line magic\")\n",
    "    return line\n",
    "\n",
    "@register_cell_magic\n",
    "def cmagic(line, cell):\n",
    "    \"my cell magic\"\n",
    "    return line, cell\n",
    "\n",
    "@register_line_cell_magic\n",
    "def lcmagic(line, cell=None):\n",
    "    \"Magic that works both as %lcmagic and as %%lcmagic\"\n",
    "    if cell is None:\n",
    "        print(\"Called as line magic\")\n",
    "        return line\n",
    "    else:\n",
    "        print(\"Called as cell magic\")\n",
    "        return line, cell\n",
    "    \n",
    "# In an interactive session, we need to delete these to avoid\n",
    "# name conflicts for automagic to work on line magics.\n",
    "# you can use %lmagic and %%lcmagic after this point\n",
    "del lmagic, lcmagic    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lmagic\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c422e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%lcmagic\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69afcd58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
